{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guangwei/miniconda3/envs/zdh/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model across multiple GPUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guangwei/miniconda3/envs/zdh/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:11<00:00,  2.38s/it]\n",
      "/tmp/ipykernel_1146258/560508729.py:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_mlp.load_state_dict(torch.load(checkpoint_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Extracting hidden states for non_infringement texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data batches: 100%|██████████| 590/590 [21:37<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract hidden states: 1297.2193 seconds\n",
      "Extracting hidden states for infringement texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data batches: 100%|██████████| 168/168 [06:12<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract hidden states: 372.8114 seconds\n",
      "Predicting on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting samples: 100%|██████████| 758/758 [00:00<00:00, 3391.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction time per sample: 0.000209 seconds\n",
      "Average total time per sample (extraction + prediction): 2.203416 seconds\n",
      "Test Accuracy: 79.82%\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Infringement       0.56      0.42      0.48       168\n",
      "Non-Infringement       0.85      0.91      0.87       590\n",
      "\n",
      "        accuracy                           0.80       758\n",
      "       macro avg       0.70      0.66      0.68       758\n",
      "    weighted avg       0.78      0.80      0.79       758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch import nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\"\n",
    "\n",
    "# Define the trained CustomMLP model\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.down = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.up = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_output = self.down(x)\n",
    "        gate_output = self.gate(x)\n",
    "        gated_output = down_output * self.activation(gate_output)\n",
    "        return self.up(gated_output)\n",
    "\n",
    "# Load a large model and distribute it across multiple GPUs\n",
    "def load_large_model(model_name):\n",
    "    \"\"\"Load a large model and distribute it across multiple GPUs.\"\"\"\n",
    "    print(\"Loading model across multiple GPUs...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"balanced\",  # Automatically balance across multiple GPUs\n",
    "        offload_folder=\"offload\",  # Offload parts of the model to disk if memory is insufficient\n",
    "        offload_state_dict=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "# Extract the hidden states of texts and measure execution time\n",
    "def extract_hidden_states(texts, model, tokenizer, batch_size=4):\n",
    "    hidden_states = []\n",
    "    \n",
    "    # Record the start time for hidden state extraction\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing data batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Access the hidden state of the last token in the last hidden layer\n",
    "        last_layer_hidden_states = outputs.hidden_states[-1]\n",
    "        last_token_hidden_states = last_layer_hidden_states[:, -1, :]  # -1 means the last token\n",
    "        hidden_states.append(last_token_hidden_states.cpu().numpy())  # Ensure the data is on CPU\n",
    "    \n",
    "    # Record the end time for hidden state extraction\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the time taken for hidden state extraction\n",
    "    extract_time = end_time - start_time\n",
    "    print(f\"Time taken to extract hidden states: {extract_time:.4f} seconds\")\n",
    "    \n",
    "    return np.vstack(hidden_states), extract_time\n",
    "\n",
    "# Prediction function\n",
    "def predict_model(model, X_test, threshold=0.5):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # Ensure the data is on the same device\n",
    "    \n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test_tensor)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        predictions = (probabilities > threshold).float().cpu().numpy()  # Move results to CPU and convert to NumPy\n",
    "    \n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate prediction time\n",
    "    prediction_time = end_time - start_time\n",
    "    return predictions, prediction_time\n",
    "\n",
    "# Load all data\n",
    "def load_all_data(non_infringement_file, infringement_file):\n",
    "    with open(non_infringement_file, 'r', encoding='utf-8') as file:\n",
    "        non_infringement_json_data = json.load(file)\n",
    "    non_infringement_outputs = [entry['input'] for entry in non_infringement_json_data]\n",
    "    y_non_infringement = [1] * len(non_infringement_outputs)\n",
    "\n",
    "    with open(infringement_file, 'r', encoding='utf-8') as file:\n",
    "        infringement_json_data = json.load(file)\n",
    "    infringement_outputs = [entry['input'] for entry in infringement_json_data]\n",
    "    y_infringement = [0] * len(infringement_outputs)\n",
    "\n",
    "    return non_infringement_outputs, y_non_infringement, infringement_outputs, y_infringement\n",
    "\n",
    "# Main function\n",
    "def main(non_infringement_file, infringement_file, checkpoint_path, model_name, batch_size=4):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "    model = load_large_model(model_name)\n",
    "    \n",
    "    # Handle padding issues\n",
    "    if tokenizer.pad_token is None:\n",
    "        # Manually add pad_token\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    # Use eos_token as pad_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set eos_token as pad_token\n",
    "    \n",
    "    # Load the trained CustomMLP\n",
    "    custom_mlp = CustomMLP(input_dim=8192, hidden_dim=256)  # Modify input dimensions as needed\n",
    "    custom_mlp.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # Set device to ensure both model and data are on the same device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    custom_mlp.to(device)\n",
    "\n",
    "    # Load all data\n",
    "    non_infringement_outputs, y_non_infringement, infringement_outputs, y_infringement = load_all_data(non_infringement_file, infringement_file)\n",
    "\n",
    "    # Extract hidden states of the texts\n",
    "    print(\"Extracting hidden states for non_infringement texts...\")\n",
    "    X_non_infringement, extract_time_non_infringement = extract_hidden_states(non_infringement_outputs, model, tokenizer, batch_size)\n",
    "\n",
    "    print(\"Extracting hidden states for infringement texts...\")\n",
    "    X_infringement, extract_time_infringement = extract_hidden_states(infringement_outputs, model, tokenizer, batch_size)\n",
    "\n",
    "    # Combine the data\n",
    "    X_test = np.vstack((X_non_infringement, X_infringement))\n",
    "    y_test = np.concatenate((y_non_infringement, y_infringement))\n",
    "\n",
    "    # Track total prediction time\n",
    "    total_prediction_time = 0\n",
    "    total_samples = len(X_test)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    print(\"Predicting on test set...\")\n",
    "    predictions = []\n",
    "    for i in tqdm(range(total_samples), desc=\"Predicting samples\"):\n",
    "        single_sample = X_test[i:i+1]  # Predict one sample at a time\n",
    "        single_prediction, prediction_time = predict_model(custom_mlp, single_sample, threshold=0.5)\n",
    "        predictions.append(single_prediction)\n",
    "        total_prediction_time += prediction_time\n",
    "\n",
    "    # Calculate average prediction time per sample\n",
    "    average_prediction_time = total_prediction_time / total_samples\n",
    "    print(f\"Average prediction time per sample: {average_prediction_time:.6f} seconds\")\n",
    "\n",
    "    # Calculate total average time (extraction + prediction)\n",
    "    total_time = extract_time_non_infringement + extract_time_infringement + total_prediction_time\n",
    "    average_total_time = total_time / total_samples\n",
    "    print(f\"Average total time per sample (extraction + prediction): {average_total_time:.6f} seconds\")\n",
    "\n",
    "    # Print results\n",
    "    predictions = np.concatenate(predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, predictions, target_names=[\"Infringement\", \"Non-Infringement\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    non_infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/literal.non_infringement.json'\n",
    "    infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/literal.infringement.json'\n",
    "    checkpoint_path = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/models/0.pth'\n",
    "    model_name = '/raid/data/guangwei/huggingface/hub/models--meta-llama--Llama-3.1-70B/snapshots/349b2ddb53ce8f2849a6c168a81980ab25258dac/'\n",
    "\n",
    "    main(non_infringement_file, infringement_file, checkpoint_path, model_name, batch_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
