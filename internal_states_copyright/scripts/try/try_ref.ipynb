{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "def load_large_model(model_name, hf_token):\n",
    "    \"\"\"Load a large model across multiple GPUs\"\"\"\n",
    "    print(\"Loading model across multiple GPUs...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"balanced\",  # Automatically distribute across multiple GPUs\n",
    "        offload_folder=\"offload\",  # Offload to disk if memory is insufficient\n",
    "        offload_state_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        use_auth_token=hf_token\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"Read data from file and return inputs, references, outputs, and labels.\"\"\"\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    inputs = [item['input'] for item in data]\n",
    "    references = [item['reference'] for item in data]\n",
    "    outputs = [item['output'] for item in data]\n",
    "    labels = [item['label'] for item in data]\n",
    "    return inputs, references, outputs, labels\n",
    "\n",
    "def main(file_path1, file_path2, threshold, model_id, hf_token):\n",
    "    \"\"\"Main program to perform label prediction and evaluation.\"\"\"\n",
    "    # Load tokenizer and large model with multi-GPU support\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = load_large_model(model_id, hf_token)\n",
    "\n",
    "    # Read data from files\n",
    "    inputs1, references1, outputs1, labels1 = read_data(file_path1)\n",
    "    inputs2, references2, outputs2, labels2 = read_data(file_path2)\n",
    "\n",
    "    merged_inputs = inputs1 + inputs2\n",
    "    merged_references = references1 + references2\n",
    "    merged_outputs = outputs1 + outputs2\n",
    "    merged_labels = labels1 + labels2\n",
    "\n",
    "    merged_data = list(zip(merged_inputs, merged_references, merged_outputs, merged_labels))\n",
    "    random.shuffle(merged_data)\n",
    "    inputs, references, outputs, truelabels = zip(*merged_data)\n",
    "\n",
    "    total_time = 0\n",
    "    num_samples = len(inputs)\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    generate_labels = []\n",
    "\n",
    "    for input_text, reference, output, truelabel in tqdm(zip(inputs, references, outputs, truelabels), total=len(inputs), desc=\"Processing data\"):\n",
    "        start_time = time.time()\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                Please **only** output 'label: 0' or 'label: 1', nothing else. Do not provide the intermediate steps; simply output the generated label.\n",
    "\n",
    "                You are a language model that evaluates whether the reference text potentially involves infringement when used to generate a continuation based on the input text.\n",
    "                - If you believe using the reference to generate the continuation would potentially involve infringement, return \"label: 0\".\n",
    "                - If you believe using the reference to generate the continuation would not involve infringement, return \"label: 1\".\n",
    "\n",
    "                Here are the inputs to evaluate:\n",
    "                Input text: '{input_text}'\n",
    "                Reference: '{reference}' (This is a segment from the original source text corresponding to the input text.)\n",
    "\n",
    "                Based on the relationship between the reference and the continuation generated from the input, provide your decision.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "        formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(next(model.parameters()).device)\n",
    "\n",
    "        generated_output = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            max_new_tokens=10,\n",
    "            num_return_sequences=1,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        predicted_labels = tokenizer.decode(generated_output[0], skip_special_tokens=True).strip().split(\":\")[-1].strip()\n",
    "        final_label_int = int(predicted_labels) if predicted_labels in ['0', '1'] else random.choice([0, 1])\n",
    "        generate_labels.append(final_label_int)\n",
    "\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "    f1 = f1_score(truelabels, generate_labels, average='macro')\n",
    "    acc = accuracy_score(truelabels, generate_labels)\n",
    "    avg_time = total_time / num_samples\n",
    "\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Average Processing Time per Sample: {avg_time:.4f} seconds\")\n",
    "    return f1, acc, avg_time\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.22222222222222224\n",
    "file_path1 = '/raid/data/guangwei/copyright_newVersion/test_division/literal.non_infringement.json'\n",
    "file_path2 = '/raid/data/guangwei/copyright_newVersion/test_division/literal.infringement.json'\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "hf_token = \"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\"\n",
    "\n",
    "f1, acc, avg_time = main(file_path1, file_path2, threshold, model_id, hf_token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
