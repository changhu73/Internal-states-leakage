{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm  \n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def read_data(file_path):\n",
    "    print(f\"Reading data from {file_path}...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    inputs = [item['input'] for item in data]\n",
    "    references = [item['reference'] for item in data]\n",
    "    outputs = [item['output'] for item in data]  \n",
    "    labels = [item['label'] for item in data]\n",
    "    \n",
    "    return inputs, references, outputs, labels\n",
    "\n",
    "def main(file_path1, file_path2, threshold, model_id, hf_token, device):\n",
    "\n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=hf_token).to(device)\n",
    "    print(\"Tokenizer and model loaded.\")\n",
    "\n",
    "    inputs1, references1, outputs1, labels1 = read_data(file_path1)\n",
    "    inputs2, references2, outputs2, labels2 = read_data(file_path2)\n",
    "\n",
    "    merged_inputs = inputs1 + inputs2\n",
    "    merged_references = references1 + references2\n",
    "    merged_outputs = outputs1 + outputs2\n",
    "    merged_labels = labels1 + labels2\n",
    "\n",
    "    merged_data = list(zip(merged_inputs, merged_references, merged_outputs, merged_labels))\n",
    "    random.shuffle(merged_data)\n",
    "\n",
    "    inputs, references, outputs, truelabels = zip(*merged_data)\n",
    "\n",
    "    total_time = 0\n",
    "    num_samples = len(inputs) \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    generate_labels = []\n",
    "    for input_text, reference, output, truelabel in tqdm(zip(inputs, references, outputs, truelabels), total=len(inputs), desc=\"Processing data\"):\n",
    "\n",
    "        print(f\"Reference: {reference}\\nOutput: {output}\\n\")\n",
    "\n",
    "        start_time_2 = time.time()\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                Please **only** output 'label: 0' or 'label: 1', nothing else. Do not provide the intermediate steps; simply output the generated label.\n",
    "\n",
    "                You are a language model that evaluates whether the output text potentially involves infringement based on the relationship between the reference and output sequences.\n",
    "                - If you believe the output involves infringement, return \"label: 0\".\n",
    "                - If you believe the output does not involve infringement, return \"label: 1\".\n",
    "\n",
    "                Here are the inputs to evaluate:\n",
    "                Input text: '{input_text}'\n",
    "                Reference: '{reference}' (This is a segment from the original source text corresponding to the input text.)\n",
    "                Output: '{output}' (This is a continuation generated based on the input text.)\n",
    "\n",
    "                Based on the comparison between the reference and output above, provide your decision.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        generated_output = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            max_new_tokens=10,\n",
    "            num_return_sequences=1,\n",
    "            repetition_penalty=1.1,\n",
    "            temperature = 0.0,\n",
    "            top_p=1.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        # Decode the generated output\n",
    "        predicted_labels = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean the output to only get 'label:0' or 'label:1'\n",
    "        # Strip anything before the label (in case there is any extra text)\n",
    "        cleaned_label = predicted_labels.strip().split(\"\\n\")[-1]  # Keep the last line, which should be the label\n",
    "\n",
    "        # Extract the last character (which should be 0 or 1)\n",
    "        final_label = cleaned_label.split(\":\")[-1].strip()  # Split by \":\", take the last part, and remove any extra spaces\n",
    "\n",
    "        # Debugging: Check the predicted label\n",
    "        print(f\"Predicted Label: {final_label}\")\n",
    "\n",
    "        # Ensure that final_label is either '0' or '1'; if not, set it to '0'\n",
    "        if final_label in ['0', '1']:\n",
    "            final_label_int = int(final_label)\n",
    "        else:\n",
    "            final_label_int = random.choice([0, 1]) \n",
    "\n",
    "        # Append the final label to the list of generated labels\n",
    "        generate_labels.append(final_label_int)\n",
    "\n",
    "        # Calculate the processing time for this step\n",
    "        end_time_2 = time.time()\n",
    "        print(f\"Processing time for this step: {end_time_2 - start_time_2:.4f} seconds\")\n",
    "        total_time = total_time + (end_time_2 - start_time_2)\n",
    "\n",
    "    f1 = f1_score(truelabels, generate_labels, average='macro')\n",
    "    acc = accuracy_score(truelabels, generate_labels)\n",
    "\n",
    "    avg_time = total_time / num_samples\n",
    "    print(f\"Average processing time per sample: {avg_time:.4f} seconds\")\n",
    "    \n",
    "    return f1, acc, avg_time\n",
    "\n",
    "threshold = 0.22222222222222224\n",
    "\n",
    "file_path1 = ''\n",
    "file_path2 = ''\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\" \n",
    "hf_token = \"\" \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "f1, acc, avg_time = main(file_path1, file_path2, threshold, model_id, hf_token, device)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Average Processing Time per Sample: {avg_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
