{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.40222144]], Indices: [[928]]\n",
      "Reference embedding: [-0.0121234   0.05992422 -0.01780817 ...  0.0265995   0.00911881\n",
      "  0.00684574]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# Set device for CUDA\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to retrieve the most similar record's reference embedding using input text\n",
    "def get_similar_reference(input_text, db_name='rag_db.sqlite', faiss_index_path='faiss_index_quantized.index'):\n",
    "    \"\"\"\n",
    "    Retrieve the reference embedding corresponding to the most similar input embedding from the database.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_text: The input text to search for in the database.\n",
    "    - db_name: The name of the SQLite database to retrieve embeddings from.\n",
    "    - faiss_index_path: Path to the saved FAISS index for fast similarity search.\n",
    "    \n",
    "    Returns:\n",
    "    - reference_embedding: The reference embedding corresponding to the most similar input embedding.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert the input text to its embedding\n",
    "    input_vector = model.encode([input_text], convert_to_tensor=True, device=device)\n",
    "    input_vector = input_vector.cpu().numpy()  # Convert to numpy array\n",
    "    \n",
    "    # Step 2: Load the FAISS index (ensure it's the quantized index)\n",
    "    faiss_index = faiss.read_index(faiss_index_path)\n",
    "    \n",
    "    # Ensure the index is trained\n",
    "    if not faiss_index.is_trained:\n",
    "        print(\"FAISS index is not trained yet. Training the index with input vectors...\")\n",
    "        faiss_index.train(input_vector)  # Train the index if not trained\n",
    "        print(\"Index trained.\")\n",
    "\n",
    "    # Step 3: Search the FAISS index for the most similar input embedding\n",
    "    D, I = faiss_index.search(input_vector, k=1)  # k=1 to get the top 1 most similar vector\n",
    "    print(f\"Distances: {D}, Indices: {I}\")\n",
    "    \n",
    "    # Step 4: Retrieve the index of the most similar vector (convert numpy.int64 to int)\n",
    "    most_similar_index = int(I[0][0])  # Convert to normal int type to avoid datatype mismatch\n",
    "    \n",
    "    # Step 5: Retrieve the corresponding reference embedding from the database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT reference_embedding FROM documents LIMIT 1 OFFSET ?\", (most_similar_index,))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result:\n",
    "        # Convert the BLOB data (reference embedding) back to numpy array\n",
    "        reference_embedding = np.frombuffer(result[0], dtype=np.float32)\n",
    "        conn.close()\n",
    "        return reference_embedding\n",
    "    else:\n",
    "        conn.close()\n",
    "        return None  # Return None if no reference found\n",
    "\n",
    "# Example usage\n",
    "input_text = \"wall.' 'O'Brien!' said Winston, making an effort to control his voice. 'You know this is not necessary. What is it that you want me to do?' O'Brien made no direct answer. When he spoke it was in the schoolmasterish manner that he sometimes affected. He looked thoughtfully into the distance, as though he were addressing an audience somewhere behind Winston's back. 'By itself,' he said, 'pain is not always enough. There are occasions when a human being will stand out against pain, even to the point of death. But for everyone there is something unendurable--something that cannot be contemplated. Courage and cowardice are not involved. If you are falling from a height it is not cowardly to clutch at a rope. If you have come up from deep water it is not cowardly to fill your lungs with air. It is merely an instinct which cannot be destroyed. It is the same with the rats. For you, they are unendurable. They are a form\"\n",
    "reference_embedding = get_similar_reference(input_text)\n",
    "\n",
    "# Print the corresponding reference embedding\n",
    "if reference_embedding is not None:\n",
    "    print(\"Reference embedding:\", reference_embedding)\n",
    "else:\n",
    "    print(\"No matching reference found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
