{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "# Variables\n",
    "model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "non_infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/extra.non_infringement.json'\n",
    "infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/extra.infringement.json'\n",
    "checkpoint_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/models/train_input_last_token.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CustumMLP for internal states train\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.down = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.up = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_output = self.down(x)\n",
    "        gate_output = self.gate(x)\n",
    "        gated_output = down_output * self.activation(gate_output)\n",
    "        return self.up(gated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(texts, model, tokenizer, batch_size=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    hidden_states = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing data batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 访问最后一个隐藏层的最后一个token的隐藏状态\n",
    "        # hidden_states[-1]表示最后一个隐藏层，mean(dim=1)表示取所有头的均值\n",
    "        last_layer_hidden_states = outputs.hidden_states[-1]\n",
    "        last_token_hidden_states = last_layer_hidden_states[:, -1, :]  # -1表示最后一个token\n",
    "        hidden_states.append(last_token_hidden_states.cpu().numpy())\n",
    "    return np.vstack(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lode data for infringement & non infringement\n",
    "def load_data(non_infringement_file, infringement_file):\n",
    "    with open(non_infringement_file, 'r', encoding='utf-8') as file:\n",
    "        non_infringement_json_data = json.load(file)\n",
    "\n",
    "    non_infringement_outputs = [entry['input'] for entry in non_infringement_json_data]\n",
    "    y_non_infringement = [1] * len(non_infringement_outputs)\n",
    "\n",
    "    with open(infringement_file, 'r', encoding='utf-8') as file:\n",
    "        infringement_json_data = json.load(file)\n",
    "\n",
    "    infringement_outputs = [entry['input'] for entry in infringement_json_data]\n",
    "    y_infringement = [0] * len(infringement_outputs)\n",
    "\n",
    "    return non_infringement_outputs, y_non_infringement, infringement_outputs, y_infringement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for best model\n",
    "def train_model(X_train, y_train, X_test, y_test, input_dim, hidden_dim, epochs=500, lr=0.001, checkpoint_path=checkpoint_file):\n",
    "    custom_mlp = CustomMLP(input_dim, hidden_dim)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(custom_mlp.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    best_accuracy = -float('inf')  # Initialize the best accuracy to negative infinity\n",
    "    best_model_state = None  # Store the state of the best model\n",
    "    best_epoch = 0  # Track the epoch with the best accuracy\n",
    "    losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        custom_mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = custom_mlp(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            custom_mlp.eval()\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                y_pred_logits = custom_mlp(X_test_tensor)\n",
    "                y_pred = (torch.sigmoid(y_pred_logits) > 0.5).float().numpy()\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            print(f\"Test Accuracy at Epoch {epoch + 1}: {accuracy * 100:.2f}%\")\n",
    "            \n",
    "            report = classification_report(y_test, y_pred, target_names=[\"infringement\", \"non_infringement\"])\n",
    "            print(f\"Classification Report at Epoch {epoch + 1}:\\n{report}\")\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model_state = custom_mlp.state_dict()\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(best_model_state, checkpoint_path)\n",
    "                print(f\"New best model saved with accuracy {best_accuracy * 100:.2f}% at epoch {best_epoch}\")\n",
    "                print(f\"Best Classification Report at Epoch {best_epoch}:\\n{report}\")\n",
    "\n",
    "    custom_mlp.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best Model was saved at epoch {best_epoch} with accuracy {best_accuracy * 100:.2f}%\")\n",
    "    return custom_mlp, losses, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "non_infringement_outputs, y_non_infringement, infringement_outputs, y_infringement = load_data(non_infringement_file, infringement_file)\n",
    "\n",
    "y_non_infringement = np.array(y_non_infringement)\n",
    "y_infringement = np.array(y_infringement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting hidden states for non_infringement texts...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.35 GiB of which 189.19 MiB is free. Process 1340726 has 68.49 GiB memory in use. Including non-PyTorch memory, this process has 10.67 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 1.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting hidden states for non_infringement texts...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X_non_infringement \u001b[38;5;241m=\u001b[39m \u001b[43mextract_hidden_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_infringement_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting hidden states for infringement texts...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m X_infringement \u001b[38;5;241m=\u001b[39m extract_hidden_states(infringement_outputs, model, tokenizer)\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mextract_hidden_states\u001b[0;34m(texts, model, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_hidden_states\u001b[39m(texts, model, tokenizer, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      2\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model)\n\u001b[1;32m      5\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/transformers/modeling_utils.py:2905\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2904\u001b[0m         )\n\u001b[0;32m-> 2905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sit/lib/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.35 GiB of which 189.19 MiB is free. Process 1340726 has 68.49 GiB memory in use. Including non-PyTorch memory, this process has 10.67 GiB memory in use. Of the allocated memory 9.64 GiB is allocated by PyTorch, and 1.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(\"Extracting hidden states for non_infringement texts...\")\n",
    "X_non_infringement = extract_hidden_states(non_infringement_outputs, model, tokenizer)\n",
    "\n",
    "print(\"Extracting hidden states for infringement texts...\")\n",
    "X_infringement = extract_hidden_states(infringement_outputs, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into training and test sets.\n"
     ]
    }
   ],
   "source": [
    "split_index_non_infringement = int(0.8 * len(X_non_infringement))\n",
    "X_non_infringement_train = X_non_infringement[:split_index_non_infringement]\n",
    "X_non_infringement_test = X_non_infringement[split_index_non_infringement:]\n",
    "y_non_infringement_train = y_non_infringement[:split_index_non_infringement]\n",
    "y_non_infringement_test = y_non_infringement[split_index_non_infringement:]\n",
    "\n",
    "split_index_infringement = int(0.8 * len(X_infringement))\n",
    "X_infringement_train = X_infringement[:split_index_infringement]\n",
    "X_infringement_test = X_infringement[split_index_infringement:]\n",
    "y_infringement_train = y_infringement[:split_index_infringement]\n",
    "y_infringement_test = y_infringement[split_index_infringement:]\n",
    "\n",
    "X_train = np.vstack((X_non_infringement_train, X_infringement_train))\n",
    "X_test = np.vstack((X_non_infringement_test, X_infringement_test))\n",
    "y_train = np.concatenate((y_non_infringement_train, y_infringement_train))\n",
    "y_test = np.concatenate((y_non_infringement_test, y_infringement_test))\n",
    "\n",
    "print(\"Data successfully split into training and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/500 [00:00<?, ?it/s]/home/guangwei/anaconda3/envs/sit/lib/python3.8/site-packages/torch/autograd/graph.py:768: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training Epochs:   2%|▏         | 10/500 [00:03<01:52,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Loss: 0.5606\n",
      "Test Accuracy at Epoch 10: 51.98%\n",
      "Classification Report at Epoch 10:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.50      0.83      0.63       283\n",
      "non_infringement       0.58      0.22      0.32       298\n",
      "\n",
      "        accuracy                           0.52       581\n",
      "       macro avg       0.54      0.53      0.48       581\n",
      "    weighted avg       0.54      0.52      0.47       581\n",
      "\n",
      "New best model saved with accuracy 51.98% at epoch 10\n",
      "Best Classification Report at Epoch 10:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.50      0.83      0.63       283\n",
      "non_infringement       0.58      0.22      0.32       298\n",
      "\n",
      "        accuracy                           0.52       581\n",
      "       macro avg       0.54      0.53      0.48       581\n",
      "    weighted avg       0.54      0.52      0.47       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   4%|▍         | 20/500 [00:06<03:17,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500, Loss: 0.5081\n",
      "Test Accuracy at Epoch 20: 64.37%\n",
      "Classification Report at Epoch 20:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.75      0.41      0.53       283\n",
      "non_infringement       0.61      0.87      0.71       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.68      0.64      0.62       581\n",
      "    weighted avg       0.67      0.64      0.62       581\n",
      "\n",
      "New best model saved with accuracy 64.37% at epoch 20\n",
      "Best Classification Report at Epoch 20:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.75      0.41      0.53       283\n",
      "non_infringement       0.61      0.87      0.71       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.68      0.64      0.62       581\n",
      "    weighted avg       0.67      0.64      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   6%|▌         | 30/500 [00:10<03:22,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500, Loss: 0.5207\n",
      "Test Accuracy at Epoch 30: 58.69%\n",
      "Classification Report at Epoch 30:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.56      0.67      0.61       283\n",
      "non_infringement       0.62      0.51      0.56       298\n",
      "\n",
      "        accuracy                           0.59       581\n",
      "       macro avg       0.59      0.59      0.59       581\n",
      "    weighted avg       0.59      0.59      0.58       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   8%|▊         | 40/500 [00:15<03:34,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/500, Loss: 0.4916\n",
      "Test Accuracy at Epoch 40: 63.68%\n",
      "Classification Report at Epoch 40:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.69      0.46      0.55       283\n",
      "non_infringement       0.61      0.80      0.69       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.65      0.63      0.62       581\n",
      "    weighted avg       0.65      0.64      0.63       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 50/500 [00:16<00:44, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500, Loss: 0.4650\n",
      "Test Accuracy at Epoch 50: 67.47%\n",
      "Classification Report at Epoch 50:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.73      0.53      0.62       283\n",
      "non_infringement       0.65      0.81      0.72       298\n",
      "\n",
      "        accuracy                           0.67       581\n",
      "       macro avg       0.69      0.67      0.67       581\n",
      "    weighted avg       0.69      0.67      0.67       581\n",
      "\n",
      "New best model saved with accuracy 67.47% at epoch 50\n",
      "Best Classification Report at Epoch 50:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.73      0.53      0.62       283\n",
      "non_infringement       0.65      0.81      0.72       298\n",
      "\n",
      "        accuracy                           0.67       581\n",
      "       macro avg       0.69      0.67      0.67       581\n",
      "    weighted avg       0.69      0.67      0.67       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  12%|█▏        | 60/500 [00:20<02:54,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500, Loss: 0.4443\n",
      "Test Accuracy at Epoch 60: 68.16%\n",
      "Classification Report at Epoch 60:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.72      0.57      0.64       283\n",
      "non_infringement       0.66      0.79      0.72       298\n",
      "\n",
      "        accuracy                           0.68       581\n",
      "       macro avg       0.69      0.68      0.68       581\n",
      "    weighted avg       0.69      0.68      0.68       581\n",
      "\n",
      "New best model saved with accuracy 68.16% at epoch 60\n",
      "Best Classification Report at Epoch 60:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.72      0.57      0.64       283\n",
      "non_infringement       0.66      0.79      0.72       298\n",
      "\n",
      "        accuracy                           0.68       581\n",
      "       macro avg       0.69      0.68      0.68       581\n",
      "    weighted avg       0.69      0.68      0.68       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  14%|█▍        | 70/500 [00:22<01:11,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500, Loss: 0.4250\n",
      "Test Accuracy at Epoch 70: 66.09%\n",
      "Classification Report at Epoch 70:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.67      0.59      0.63       283\n",
      "non_infringement       0.65      0.73      0.69       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.66      0.66       581\n",
      "    weighted avg       0.66      0.66      0.66       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  16%|█▌        | 80/500 [00:26<02:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500, Loss: 0.4061\n",
      "Test Accuracy at Epoch 80: 67.13%\n",
      "Classification Report at Epoch 80:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.69      0.60      0.64       283\n",
      "non_infringement       0.66      0.74      0.70       298\n",
      "\n",
      "        accuracy                           0.67       581\n",
      "       macro avg       0.67      0.67      0.67       581\n",
      "    weighted avg       0.67      0.67      0.67       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  18%|█▊        | 90/500 [00:29<02:23,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/500, Loss: 0.3891\n",
      "Test Accuracy at Epoch 90: 64.03%\n",
      "Classification Report at Epoch 90:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.59      0.62       283\n",
      "non_infringement       0.64      0.68      0.66       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|█▉        | 99/500 [00:31<02:18,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500, Loss: 0.3741\n",
      "Test Accuracy at Epoch 100: 64.37%\n",
      "Classification Report at Epoch 100:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.60      0.62       283\n",
      "non_infringement       0.64      0.69      0.66       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  22%|██▏       | 110/500 [00:38<04:26,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/500, Loss: 0.3603\n",
      "Test Accuracy at Epoch 110: 62.31%\n",
      "Classification Report at Epoch 110:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.62      0.57      0.60       283\n",
      "non_infringement       0.62      0.67      0.65       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.62      0.62      0.62       581\n",
      "    weighted avg       0.62      0.62      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  24%|██▍       | 120/500 [00:42<03:15,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500, Loss: 0.4302\n",
      "Test Accuracy at Epoch 120: 66.61%\n",
      "Classification Report at Epoch 120:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.79      0.43      0.56       283\n",
      "non_infringement       0.62      0.89      0.73       298\n",
      "\n",
      "        accuracy                           0.67       581\n",
      "       macro avg       0.70      0.66      0.64       581\n",
      "    weighted avg       0.70      0.67      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  26%|██▌       | 130/500 [00:47<03:07,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/500, Loss: 0.3972\n",
      "Test Accuracy at Epoch 130: 61.62%\n",
      "Classification Report at Epoch 130:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.60      0.63      0.62       283\n",
      "non_infringement       0.63      0.60      0.62       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.62      0.62      0.62       581\n",
      "    weighted avg       0.62      0.62      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  28%|██▊       | 140/500 [00:50<01:06,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/500, Loss: 0.3760\n",
      "Test Accuracy at Epoch 140: 65.06%\n",
      "Classification Report at Epoch 140:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.61      0.63       283\n",
      "non_infringement       0.65      0.69      0.67       298\n",
      "\n",
      "        accuracy                           0.65       581\n",
      "       macro avg       0.65      0.65      0.65       581\n",
      "    weighted avg       0.65      0.65      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 150/500 [00:54<02:44,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/500, Loss: 0.3529\n",
      "Test Accuracy at Epoch 150: 66.44%\n",
      "Classification Report at Epoch 150:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.71      0.53      0.60       283\n",
      "non_infringement       0.64      0.80      0.71       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.67      0.66      0.66       581\n",
      "    weighted avg       0.67      0.66      0.66       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  32%|███▏      | 160/500 [00:58<01:41,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/500, Loss: 0.3414\n",
      "Test Accuracy at Epoch 160: 64.03%\n",
      "Classification Report at Epoch 160:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.61      0.62       283\n",
      "non_infringement       0.64      0.66      0.65       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  34%|███▍      | 170/500 [01:03<02:51,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/500, Loss: 0.3321\n",
      "Test Accuracy at Epoch 170: 61.79%\n",
      "Classification Report at Epoch 170:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.62      0.57      0.59       283\n",
      "non_infringement       0.62      0.67      0.64       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.62      0.62      0.62       581\n",
      "    weighted avg       0.62      0.62      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  36%|███▌      | 180/500 [01:08<02:59,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/500, Loss: 0.3230\n",
      "Test Accuracy at Epoch 180: 65.92%\n",
      "Classification Report at Epoch 180:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.68      0.57      0.62       283\n",
      "non_infringement       0.64      0.75      0.69       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.66      0.66       581\n",
      "    weighted avg       0.66      0.66      0.66       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  38%|███▊      | 190/500 [01:13<02:19,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/500, Loss: 0.3144\n",
      "Test Accuracy at Epoch 190: 65.92%\n",
      "Classification Report at Epoch 190:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.69      0.55      0.61       283\n",
      "non_infringement       0.64      0.76      0.70       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.66      0.65       581\n",
      "    weighted avg       0.66      0.66      0.66       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 200/500 [01:19<02:24,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/500, Loss: 0.4841\n",
      "Test Accuracy at Epoch 200: 61.27%\n",
      "Classification Report at Epoch 200:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.61      0.56      0.59       283\n",
      "non_infringement       0.61      0.66      0.64       298\n",
      "\n",
      "        accuracy                           0.61       581\n",
      "       macro avg       0.61      0.61      0.61       581\n",
      "    weighted avg       0.61      0.61      0.61       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  42%|████▏     | 210/500 [01:23<02:28,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/500, Loss: 0.3250\n",
      "Test Accuracy at Epoch 210: 65.75%\n",
      "Classification Report at Epoch 210:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.73      0.47      0.57       283\n",
      "non_infringement       0.62      0.83      0.71       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.68      0.65      0.64       581\n",
      "    weighted avg       0.68      0.66      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  44%|████▍     | 220/500 [01:29<02:25,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/500, Loss: 0.3227\n",
      "Test Accuracy at Epoch 220: 64.37%\n",
      "Classification Report at Epoch 220:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.61      0.62       283\n",
      "non_infringement       0.65      0.68      0.66       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  46%|████▌     | 229/500 [01:34<02:47,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/500, Loss: 0.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  46%|████▌     | 230/500 [01:35<03:31,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy at Epoch 230: 65.75%\n",
      "Classification Report at Epoch 230:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.69      0.55      0.61       283\n",
      "non_infringement       0.64      0.76      0.70       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.65      0.65       581\n",
      "    weighted avg       0.66      0.66      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  48%|████▊     | 240/500 [01:40<01:57,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500, Loss: 0.2972\n",
      "Test Accuracy at Epoch 240: 65.58%\n",
      "Classification Report at Epoch 240:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.68      0.55      0.61       283\n",
      "non_infringement       0.64      0.75      0.69       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.65      0.65       581\n",
      "    weighted avg       0.66      0.66      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|█████     | 250/500 [01:44<01:48,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/500, Loss: 0.2901\n",
      "Test Accuracy at Epoch 250: 65.23%\n",
      "Classification Report at Epoch 250:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.68      0.55      0.61       283\n",
      "non_infringement       0.64      0.75      0.69       298\n",
      "\n",
      "        accuracy                           0.65       581\n",
      "       macro avg       0.66      0.65      0.65       581\n",
      "    weighted avg       0.66      0.65      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  52%|█████▏    | 260/500 [01:48<01:18,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/500, Loss: 0.2797\n",
      "Test Accuracy at Epoch 260: 64.37%\n",
      "Classification Report at Epoch 260:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.66      0.57      0.61       283\n",
      "non_infringement       0.64      0.72      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.65      0.64      0.64       581\n",
      "    weighted avg       0.65      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  54%|█████▍    | 270/500 [01:54<03:01,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/500, Loss: 0.4631\n",
      "Test Accuracy at Epoch 270: 62.82%\n",
      "Classification Report at Epoch 270:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.53      0.58       283\n",
      "non_infringement       0.62      0.72      0.67       298\n",
      "\n",
      "        accuracy                           0.63       581\n",
      "       macro avg       0.63      0.63      0.62       581\n",
      "    weighted avg       0.63      0.63      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  56%|█████▌    | 280/500 [02:01<02:28,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280/500, Loss: 0.3439\n",
      "Test Accuracy at Epoch 280: 67.47%\n",
      "Classification Report at Epoch 280:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.71      0.56      0.63       283\n",
      "non_infringement       0.65      0.78      0.71       298\n",
      "\n",
      "        accuracy                           0.67       581\n",
      "       macro avg       0.68      0.67      0.67       581\n",
      "    weighted avg       0.68      0.67      0.67       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  58%|█████▊    | 290/500 [02:05<01:33,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500, Loss: 0.2934\n",
      "Test Accuracy at Epoch 290: 64.03%\n",
      "Classification Report at Epoch 290:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.63      0.63      0.63       283\n",
      "non_infringement       0.65      0.65      0.65       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 300/500 [02:09<01:26,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500, Loss: 0.2829\n",
      "Test Accuracy at Epoch 300: 65.58%\n",
      "Classification Report at Epoch 300:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.68      0.56      0.61       283\n",
      "non_infringement       0.64      0.75      0.69       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.65      0.65       581\n",
      "    weighted avg       0.66      0.66      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  62%|██████▏   | 310/500 [02:14<01:24,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310/500, Loss: 0.2727\n",
      "Test Accuracy at Epoch 310: 64.20%\n",
      "Classification Report at Epoch 310:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.66      0.55      0.60       283\n",
      "non_infringement       0.63      0.72      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  64%|██████▍   | 320/500 [02:18<01:17,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320/500, Loss: 0.2634\n",
      "Test Accuracy at Epoch 320: 64.54%\n",
      "Classification Report at Epoch 320:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.66      0.56      0.61       283\n",
      "non_infringement       0.64      0.72      0.68       298\n",
      "\n",
      "        accuracy                           0.65       581\n",
      "       macro avg       0.65      0.64      0.64       581\n",
      "    weighted avg       0.65      0.65      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  66%|██████▌   | 330/500 [02:22<01:12,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/500, Loss: 0.2563\n",
      "Test Accuracy at Epoch 330: 64.20%\n",
      "Classification Report at Epoch 330:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.58      0.61       283\n",
      "non_infringement       0.64      0.70      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  68%|██████▊   | 340/500 [02:26<01:09,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/500, Loss: 0.2565\n",
      "Test Accuracy at Epoch 340: 63.51%\n",
      "Classification Report at Epoch 340:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.67      0.49      0.57       283\n",
      "non_infringement       0.61      0.78      0.69       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.63      0.63       581\n",
      "    weighted avg       0.64      0.64      0.63       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|███████   | 350/500 [02:30<01:07,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/500, Loss: 0.2605\n",
      "Test Accuracy at Epoch 350: 64.20%\n",
      "Classification Report at Epoch 350:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.56      0.60       283\n",
      "non_infringement       0.63      0.72      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  72%|███████▏  | 360/500 [02:35<01:05,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/500, Loss: 0.2469\n",
      "Test Accuracy at Epoch 360: 64.37%\n",
      "Classification Report at Epoch 360:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.60      0.62       283\n",
      "non_infringement       0.64      0.68      0.66       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  74%|███████▍  | 370/500 [02:40<01:01,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/500, Loss: 0.2574\n",
      "Test Accuracy at Epoch 370: 61.62%\n",
      "Classification Report at Epoch 370:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.45      0.53       283\n",
      "non_infringement       0.60      0.77      0.67       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.63      0.61      0.60       581\n",
      "    weighted avg       0.62      0.62      0.61       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  76%|███████▌  | 380/500 [02:46<01:05,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380/500, Loss: 0.2375\n",
      "Test Accuracy at Epoch 380: 64.20%\n",
      "Classification Report at Epoch 380:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.58      0.61       283\n",
      "non_infringement       0.64      0.70      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  78%|███████▊  | 390/500 [02:51<01:06,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390/500, Loss: 0.2845\n",
      "Test Accuracy at Epoch 390: 64.03%\n",
      "Classification Report at Epoch 390:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.70      0.45      0.55       283\n",
      "non_infringement       0.61      0.82      0.70       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.66      0.64      0.63       581\n",
      "    weighted avg       0.66      0.64      0.63       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 400/500 [02:56<00:51,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/500, Loss: 0.2510\n",
      "Test Accuracy at Epoch 400: 63.68%\n",
      "Classification Report at Epoch 400:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.57      0.61       283\n",
      "non_infringement       0.63      0.70      0.66       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.63       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  82%|████████▏ | 410/500 [03:00<00:40,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 410/500, Loss: 0.2323\n",
      "Test Accuracy at Epoch 410: 62.13%\n",
      "Classification Report at Epoch 410:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.50      0.56       283\n",
      "non_infringement       0.61      0.74      0.67       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.63      0.62      0.61       581\n",
      "    weighted avg       0.63      0.62      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  84%|████████▍ | 420/500 [03:03<00:25,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/500, Loss: 0.2274\n",
      "Test Accuracy at Epoch 420: 62.31%\n",
      "Classification Report at Epoch 420:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.51      0.57       283\n",
      "non_infringement       0.61      0.73      0.66       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.63      0.62      0.62       581\n",
      "    weighted avg       0.63      0.62      0.62       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  86%|████████▌ | 430/500 [03:06<00:28,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/500, Loss: 0.2266\n",
      "Test Accuracy at Epoch 430: 61.79%\n",
      "Classification Report at Epoch 430:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.46      0.54       283\n",
      "non_infringement       0.60      0.77      0.67       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.63      0.61      0.61       581\n",
      "    weighted avg       0.63      0.62      0.61       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  88%|████████▊ | 440/500 [03:09<00:10,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440/500, Loss: 0.2716\n",
      "Test Accuracy at Epoch 440: 62.31%\n",
      "Classification Report at Epoch 440:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.69      0.41      0.52       283\n",
      "non_infringement       0.60      0.82      0.69       298\n",
      "\n",
      "        accuracy                           0.62       581\n",
      "       macro avg       0.64      0.62      0.60       581\n",
      "    weighted avg       0.64      0.62      0.61       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████ | 450/500 [03:11<00:08,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450/500, Loss: 0.2585\n",
      "Test Accuracy at Epoch 450: 65.40%\n",
      "Classification Report at Epoch 450:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.69      0.53      0.60       283\n",
      "non_infringement       0.63      0.77      0.70       298\n",
      "\n",
      "        accuracy                           0.65       581\n",
      "       macro avg       0.66      0.65      0.65       581\n",
      "    weighted avg       0.66      0.65      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  93%|█████████▎| 463/500 [03:13<00:02, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460/500, Loss: 0.2282\n",
      "Test Accuracy at Epoch 460: 65.58%\n",
      "Classification Report at Epoch 460:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.66      0.61      0.63       283\n",
      "non_infringement       0.65      0.70      0.68       298\n",
      "\n",
      "        accuracy                           0.66       581\n",
      "       macro avg       0.66      0.65      0.65       581\n",
      "    weighted avg       0.66      0.66      0.65       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  94%|█████████▍| 471/500 [03:13<00:02, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/500, Loss: 0.2223\n",
      "Test Accuracy at Epoch 470: 64.03%\n",
      "Classification Report at Epoch 470:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.59      0.61       283\n",
      "non_infringement       0.64      0.69      0.66       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  97%|█████████▋| 484/500 [03:14<00:00, 22.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/500, Loss: 0.2168\n",
      "Test Accuracy at Epoch 480: 64.03%\n",
      "Classification Report at Epoch 480:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.65      0.57      0.60       283\n",
      "non_infringement       0.63      0.71      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.64      0.64       581\n",
      "    weighted avg       0.64      0.64      0.64       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  98%|█████████▊| 491/500 [03:14<00:00, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490/500, Loss: 0.2128\n",
      "Test Accuracy at Epoch 490: 63.51%\n",
      "Classification Report at Epoch 490:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.56      0.60       283\n",
      "non_infringement       0.63      0.71      0.67       298\n",
      "\n",
      "        accuracy                           0.64       581\n",
      "       macro avg       0.64      0.63      0.63       581\n",
      "    weighted avg       0.64      0.64      0.63       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 500/500 [03:17<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/500, Loss: 0.2097\n",
      "Test Accuracy at Epoch 500: 63.17%\n",
      "Classification Report at Epoch 500:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.64      0.55      0.59       283\n",
      "non_infringement       0.62      0.71      0.66       298\n",
      "\n",
      "        accuracy                           0.63       581\n",
      "       macro avg       0.63      0.63      0.63       581\n",
      "    weighted avg       0.63      0.63      0.63       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_1078087/3960048660.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_mlp.load_state_dict(torch.load(checkpoint_path))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model was saved at epoch 60 with accuracy 68.16%\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 256\n",
    "\n",
    "custom_mlp, losses, best_accuracy = train_model(X_train, y_train, X_test, y_test, input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/models/train_input_last_token.pth'.\n"
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to '{filepath}'.\")\n",
    "\n",
    "save_checkpoint(custom_mlp, torch.optim.Adam(custom_mlp.parameters()), len(losses), losses[-1], checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 68.16%\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    infringement       0.72      0.57      0.64       283\n",
      "non_infringement       0.66      0.79      0.72       298\n",
      "\n",
      "        accuracy                           0.68       581\n",
      "       macro avg       0.69      0.68      0.68       581\n",
      "    weighted avg       0.69      0.68      0.68       581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1078087/3740627605.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred_final = (torch.sigmoid(torch.tensor(custom_mlp(torch.tensor(X_test, dtype=torch.float32)))) > 0.5).float().numpy()\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final Model Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "y_pred_final = (torch.sigmoid(torch.tensor(custom_mlp(torch.tensor(X_test, dtype=torch.float32)))) > 0.5).float().numpy()\n",
    "print(classification_report(y_test, y_pred_final, target_names=[\"infringement\", \"non_infringement\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
