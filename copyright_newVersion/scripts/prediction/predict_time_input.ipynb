{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guangwei/miniconda3/envs/zdh/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/guangwei/miniconda3/envs/zdh/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n",
      "/tmp/ipykernel_3590693/297841794.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_mlp.load_state_dict(torch.load(checkpoint_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting hidden states for non_infringement texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data batches: 100%|██████████| 743/743 [00:43<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract hidden states: 43.4474 seconds\n",
      "Extracting hidden states for infringement texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data batches: 100%|██████████| 706/706 [00:37<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to extract hidden states: 37.9508 seconds\n",
      "Predicting on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting samples: 100%|██████████| 1449/1449 [00:00<00:00, 2950.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prediction time per sample: 0.000246 seconds\n",
      "Average total time per sample (extraction + prediction): 0.056421 seconds\n",
      "Test Accuracy: 66.32%\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Infringement       0.77      0.44      0.56       706\n",
      "Non-Infringement       0.62      0.88      0.73       743\n",
      "\n",
      "        accuracy                           0.66      1449\n",
      "       macro avg       0.70      0.66      0.64      1449\n",
      "    weighted avg       0.69      0.66      0.65      1449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch import nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 使用 GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# 加载训练好的 CustomMLP\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.down = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.up = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_output = self.down(x)\n",
    "        gate_output = self.gate(x)\n",
    "        gated_output = down_output * self.activation(gate_output)\n",
    "        return self.up(gated_output)\n",
    "\n",
    "# 提取文本的内部状态，并计算时间\n",
    "def extract_hidden_states(texts, model, tokenizer, batch_size=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # 确保模型在相同设备上\n",
    "    model = nn.DataParallel(model)  # 多GPU支持\n",
    "    hidden_states = []\n",
    "    \n",
    "    # 记录提取hidden states的开始时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing data batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        # 将inputs的所有tensor移动到相同的设备\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # 访问最后一个隐藏层的最后一个token的隐藏状态\n",
    "        last_layer_hidden_states = outputs.hidden_states[-1]\n",
    "        last_token_hidden_states = last_layer_hidden_states[:, -1, :]  # -1表示最后一个token\n",
    "        hidden_states.append(last_token_hidden_states.cpu().numpy())  # 确保数据在CPU上\n",
    "    \n",
    "    # 记录提取hidden states的结束时间\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 计算提取hidden states的时间\n",
    "    extract_time = end_time - start_time\n",
    "    print(f\"Time taken to extract hidden states: {extract_time:.4f} seconds\")\n",
    "    \n",
    "    return np.vstack(hidden_states), extract_time\n",
    "\n",
    "# 预测方法\n",
    "def predict_model(model, X_test, threshold=0.5):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # 确保数据在相同设备上\n",
    "    \n",
    "    # 记录开始时间\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test_tensor)\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        predictions = (probabilities > threshold).float().cpu().numpy()  # 先将结果移到 CPU 然后转换为 NumPy\n",
    "    \n",
    "    # 记录结束时间\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 计算预测时间\n",
    "    prediction_time = end_time - start_time\n",
    "    return predictions, prediction_time\n",
    "\n",
    "# 加载所有数据\n",
    "def load_all_data(non_infringement_file, infringement_file):\n",
    "    with open(non_infringement_file, 'r', encoding='utf-8') as file:\n",
    "        non_infringement_json_data = json.load(file)\n",
    "    non_infringement_outputs = [entry['input'] for entry in non_infringement_json_data]\n",
    "    y_non_infringement = [1] * len(non_infringement_outputs)\n",
    "\n",
    "    with open(infringement_file, 'r', encoding='utf-8') as file:\n",
    "        infringement_json_data = json.load(file)\n",
    "    infringement_outputs = [entry['input'] for entry in infringement_json_data]\n",
    "    y_infringement = [0] * len(infringement_outputs)\n",
    "\n",
    "    return non_infringement_outputs, y_non_infringement, infringement_outputs, y_infringement\n",
    "\n",
    "# 主程序\n",
    "def main(non_infringement_file, infringement_file, checkpoint_path, model_name, batch_size=4):\n",
    "    # 加载 tokenizer 和模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "    \n",
    "    # 解决 padding 问题\n",
    "    if tokenizer.pad_token is None:\n",
    "        # 手动添加 pad_token\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    # 使用 eos_token 作为 pad_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # 设置 eos_token 作为 pad_token\n",
    "    \n",
    "    # 加载训练好的 CustomMLP\n",
    "    custom_mlp = CustomMLP(input_dim=4096, hidden_dim=256)  # 根据实际输入维度修改\n",
    "    custom_mlp.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    # 设备设置，确保模型和数据都在同一设备上\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    custom_mlp.to(device)\n",
    "\n",
    "    # 加载所有数据\n",
    "    non_infringement_outputs, y_non_infringement, infringement_outputs, y_infringement = load_all_data(non_infringement_file, infringement_file)\n",
    "\n",
    "    # 提取文本的内部状态\n",
    "    print(\"Extracting hidden states for non_infringement texts...\")\n",
    "    X_non_infringement, extract_time_non_infringement = extract_hidden_states(non_infringement_outputs, model, tokenizer, batch_size)\n",
    "\n",
    "    print(\"Extracting hidden states for infringement texts...\")\n",
    "    X_infringement, extract_time_infringement = extract_hidden_states(infringement_outputs, model, tokenizer, batch_size)\n",
    "\n",
    "    # 组合数据\n",
    "    X_test = np.vstack((X_non_infringement, X_infringement))\n",
    "    y_test = np.concatenate((y_non_infringement, y_infringement))\n",
    "\n",
    "    # 记录总时间\n",
    "    total_prediction_time = 0\n",
    "    total_samples = len(X_test)\n",
    "\n",
    "    # 使用训练好的模型进行预测\n",
    "    print(\"Predicting on test set...\")\n",
    "    predictions = []\n",
    "    for i in tqdm(range(total_samples), desc=\"Predicting samples\"):\n",
    "        single_sample = X_test[i:i+1]  # 预测单条数据\n",
    "        single_prediction, prediction_time = predict_model(custom_mlp, single_sample, threshold=0.5)\n",
    "        predictions.append(single_prediction)\n",
    "        total_prediction_time += prediction_time\n",
    "\n",
    "    # 计算平均预测时间\n",
    "    average_prediction_time = total_prediction_time / total_samples\n",
    "    print(f\"Average prediction time per sample: {average_prediction_time:.6f} seconds\")\n",
    "\n",
    "    # 计算总的平均时间\n",
    "    total_time = extract_time_non_infringement + extract_time_infringement + total_prediction_time\n",
    "    average_total_time = total_time / total_samples\n",
    "    print(f\"Average total time per sample (extraction + prediction): {average_total_time:.6f} seconds\")\n",
    "\n",
    "    # 打印结果\n",
    "    predictions = np.concatenate(predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, predictions, target_names=[\"Infringement\", \"Non-Infringement\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置路径\n",
    "    non_infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/extra_30.non_infringement.json'\n",
    "    infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/extra_30.infringement.json'\n",
    "    checkpoint_path = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/models/train_input_last_token.pth'\n",
    "    model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "\n",
    "    main(non_infringement_file, infringement_file, checkpoint_path, model_name, batch_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
