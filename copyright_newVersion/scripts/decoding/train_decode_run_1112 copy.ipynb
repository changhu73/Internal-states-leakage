{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages & variables\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoModelForSequenceClassification\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Parameters\n",
    "model_name = 'meta-llama/Meta-Llama-3.1-8B'\n",
    "non_infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/extra_30.non_infringement.json'\n",
    "infringement_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/test_division/extra_30.infringement.json'\n",
    "checkpoint_file = '/home/guangwei/LLM-COPYRIGHT/copyright_newVersion/models/train_input_reference_0_layer.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CustumMLP for internal states train\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.down = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gate = nn.Linear(input_dim, hidden_dim)\n",
    "        self.up = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_output = self.down(x)\n",
    "        gate_output = self.gate(x)\n",
    "        gated_output = down_output * self.activation(gate_output)\n",
    "        return self.up(gated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hidden states/reference embeddings\n",
    "def extract_hidden_states(texts, model, tokenizer, batch_size=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    hidden_states = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing data batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hidden_states.append(outputs.hidden_states[-1].mean(dim=1).cpu().numpy())\n",
    "    \n",
    "    return np.vstack(hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "def extract_decoding(references, model, tokenizer, bert_model, bert_tokenizer, max_length=300, batch_size=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    bert_model.to(device)\n",
    "    bert_model = nn.DataParallel(bert_model)\n",
    "    \n",
    "    outputs = []\n",
    "    ttft_list = []\n",
    "    tpot_list = []\n",
    "    sysprompt = \"You will be shown a series of passages from famous literary works. After these examples, you will receive a prefix from another passage and be asked to complete it based on the text of a famous work. Provide only the continuation for the last given prefix without any extra commentary, formatting, or additional text.\"\n",
    "    \n",
    "    for i in tqdm(range(0, len(references), batch_size), desc=\"Processing references\"):\n",
    "        batch_references = references[i:i + batch_size]\n",
    "        # Create the input prompt by concatenating sysprompt and the actual reference text\n",
    "        prompt = sysprompt + \" \" + batch_references[0]\n",
    "        \n",
    "        # Tokenize and prepare input for generation model\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, padding_side='left').to(device)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate sequences\n",
    "            start_time = time.time()\n",
    "            generated_ids = model.generate(\n",
    "                **inputs, max_new_tokens=20, output_hidden_states=False, return_dict_in_generate=True, use_cache=True\n",
    "            )\n",
    "            ttft = time.time() - start_time\n",
    "            num_tokens_generated = generated_ids[\"sequences\"].shape[1]\n",
    "            tpot = ttft / num_tokens_generated if num_tokens_generated > 0 else float('inf')\n",
    "\n",
    "            # Decode the generated sequences to text\n",
    "            generated_texts = tokenizer.batch_decode(generated_ids[\"sequences\"], skip_special_tokens=True)\n",
    "            \n",
    "            # Remove the prompt part from generated_texts if it's included\n",
    "            generated_texts_cleaned = []\n",
    "            for text in generated_texts:\n",
    "                # Remove the prompt portion\n",
    "                text = text[len(prompt):].strip()\n",
    "                generated_texts_cleaned.append(text)\n",
    "            \n",
    "            # Print the cleaned generated texts\n",
    "            # print(\"Generated Continuations:\", generated_texts_cleaned)\n",
    "\n",
    "            # Tokenize the cleaned text for BERT model\n",
    "            bert_inputs = bert_tokenizer(generated_texts_cleaned, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            bert_outputs = bert_model(**bert_inputs)\n",
    "            \n",
    "            # Extract BERT embeddings\n",
    "            embedding = bert_outputs.pooler_output.cpu().numpy()\n",
    "        \n",
    "        outputs.append(embedding)\n",
    "        ttft_list.append(ttft)\n",
    "        tpot_list.append(tpot)\n",
    "\n",
    "    # Calculate averages for TTFT and TPOT\n",
    "    average_ttft = np.mean(ttft_list)\n",
    "    average_tpot = np.mean(tpot_list)\n",
    "    \n",
    "    return np.vstack(outputs), average_ttft, average_tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for infringement & non infringement\n",
    "def load_data(non_infringement_file, infringement_file):\n",
    "    with open(non_infringement_file, 'r', encoding='utf-8') as file:\n",
    "        non_infringement_json_data = json.load(file)\n",
    "\n",
    "    non_infringement_outputs = [entry['input'] for entry in non_infringement_json_data]\n",
    "    non_infringement_references = [entry['reference'] for entry in non_infringement_json_data]\n",
    "    y_non_infringement = [1] * len(non_infringement_outputs)\n",
    "\n",
    "    with open(infringement_file, 'r', encoding='utf-8') as file:\n",
    "        infringement_json_data = json.load(file)\n",
    "\n",
    "    infringement_outputs = [entry['input'] for entry in infringement_json_data]\n",
    "    infringement_references = [entry['reference'] for entry in infringement_json_data]\n",
    "    y_infringement = [0] * len(infringement_outputs)\n",
    "\n",
    "    return non_infringement_outputs, non_infringement_references, y_non_infringement, infringement_outputs, infringement_references, y_infringement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "# Train for best model\n",
    "def train_model(X_train, y_train, X_test, y_test, input_dim, hidden_dim, epochs=2000, lr=0.001, checkpoint_path=checkpoint_file):\n",
    "    custom_mlp = CustomMLP(input_dim, hidden_dim)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(custom_mlp.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    best_accuracy = -float('inf')\n",
    "    best_f1 = -float('inf')  # Track best F1-score\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        custom_mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = custom_mlp(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Every 10 epochs, evaluate the model\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            custom_mlp.eval()\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                y_pred_logits = custom_mlp(X_test_tensor)\n",
    "                y_pred = (torch.sigmoid(y_pred_logits) > 0.5).float().numpy()\n",
    "            \n",
    "            # Calculate accuracy and F1-score\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)  # F1-score calculation\n",
    "            \n",
    "            print(f\"Test Accuracy at Epoch {epoch + 1}: {accuracy * 100:.2f}%\")\n",
    "            print(f\"Test F1-score at Epoch {epoch + 1}: {f1:.4f}\")\n",
    "            \n",
    "            # Generate classification report\n",
    "            report = classification_report(y_test, y_pred, target_names=[\"infringement\", \"non_infringement\"])\n",
    "            print(f\"Classification Report at Epoch {epoch + 1}:\\n{report}\")\n",
    "\n",
    "            # Save the model if it achieves a better F1-score\n",
    "            if f1 > best_f1:\n",
    "                best_accuracy = accuracy\n",
    "                best_f1 = f1\n",
    "                best_model_state = custom_mlp.state_dict()\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(best_model_state, checkpoint_path)\n",
    "                print(f\"New best model saved with F1-score {best_f1:.4f} at epoch {best_epoch}\")\n",
    "                print(f\"Best Classification Report at Epoch {best_epoch}:\\n{report}\")\n",
    "\n",
    "    # Load the best model state\n",
    "    custom_mlp.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Model Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "    print(f\"Final Model F1-score: {best_f1:.4f}\")\n",
    "    \n",
    "    return custom_mlp, losses, best_accuracy, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('google-bert/bert-base-uncased')\n",
    "bert_tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "non_infringement_outputs, non_infringement_references, y_non_infringement, infringement_outputs, infringement_references, y_infringement = load_data(\n",
    "    non_infringement_file, infringement_file\n",
    ")\n",
    "\n",
    "y_non_infringement = np.array(y_non_infringement)\n",
    "y_infringement = np.array(y_infringement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing references:  35%|███▍      | 85/243 [01:13<01:56,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  35%|███▌      | 86/243 [01:14<01:55,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  36%|███▌      | 87/243 [01:14<01:55,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  36%|███▌      | 88/243 [01:15<01:55,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  37%|███▋      | 89/243 [01:16<01:54,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  37%|███▋      | 90/243 [01:17<01:58,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  37%|███▋      | 91/243 [01:18<01:55,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  38%|███▊      | 92/243 [01:18<01:35,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  38%|███▊      | 93/243 [01:19<01:39,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  39%|███▊      | 94/243 [01:19<01:23,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  39%|███▉      | 95/243 [01:20<01:23,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  40%|███▉      | 96/243 [01:20<01:32,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  40%|███▉      | 97/243 [01:21<01:41,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  40%|████      | 98/243 [01:22<01:42,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  41%|████      | 99/243 [01:23<01:42,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  41%|████      | 100/243 [01:23<01:33,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  42%|████▏     | 101/243 [01:24<01:36,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  42%|████▏     | 102/243 [01:25<01:48,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  42%|████▏     | 103/243 [01:26<01:48,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  43%|████▎     | 104/243 [01:26<01:47,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  43%|████▎     | 105/243 [01:27<01:48,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  44%|████▎     | 106/243 [01:28<01:47,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  44%|████▍     | 107/243 [01:29<01:45,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  44%|████▍     | 108/243 [01:30<01:49,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  45%|████▍     | 109/243 [01:30<01:47,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  45%|████▌     | 110/243 [01:31<01:47,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  46%|████▌     | 111/243 [01:32<01:44,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  46%|████▌     | 112/243 [01:33<01:37,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  47%|████▋     | 113/243 [01:33<01:33,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  47%|████▋     | 114/243 [01:34<01:30,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  47%|████▋     | 115/243 [01:35<01:28,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  48%|████▊     | 116/243 [01:35<01:25,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  48%|████▊     | 117/243 [01:36<01:24,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  49%|████▊     | 118/243 [01:37<01:22,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  49%|████▉     | 119/243 [01:37<01:21,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  49%|████▉     | 120/243 [01:38<01:20,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  50%|████▉     | 121/243 [01:39<01:18,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  50%|█████     | 122/243 [01:39<01:17,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  51%|█████     | 123/243 [01:40<01:15,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  51%|█████     | 124/243 [01:40<01:15,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  51%|█████▏    | 125/243 [01:41<01:15,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  52%|█████▏    | 126/243 [01:41<01:01,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  52%|█████▏    | 127/243 [01:42<01:06,  1.76it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  53%|█████▎    | 128/243 [01:43<01:09,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  53%|█████▎    | 129/243 [01:43<01:11,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  53%|█████▎    | 130/243 [01:44<01:11,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  54%|█████▍    | 131/243 [01:45<01:11,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  54%|█████▍    | 132/243 [01:45<01:12,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  55%|█████▍    | 133/243 [01:46<01:13,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  55%|█████▌    | 134/243 [01:47<01:12,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  56%|█████▌    | 135/243 [01:47<01:11,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  56%|█████▌    | 136/243 [01:48<01:10,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  56%|█████▋    | 137/243 [01:49<01:09,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  57%|█████▋    | 138/243 [01:49<01:09,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  57%|█████▋    | 139/243 [01:50<01:09,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  58%|█████▊    | 140/243 [01:51<01:08,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  58%|█████▊    | 141/243 [01:51<01:07,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  58%|█████▊    | 142/243 [01:52<01:06,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  59%|█████▉    | 143/243 [01:53<01:04,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  59%|█████▉    | 144/243 [01:53<01:03,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  60%|█████▉    | 145/243 [01:54<01:01,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  60%|██████    | 146/243 [01:54<01:00,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  60%|██████    | 147/243 [01:55<00:59,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  61%|██████    | 148/243 [01:56<00:58,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  61%|██████▏   | 149/243 [01:56<00:58,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  62%|██████▏   | 150/243 [01:57<00:57,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  62%|██████▏   | 151/243 [01:58<00:57,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  63%|██████▎   | 152/243 [01:58<00:56,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  63%|██████▎   | 153/243 [01:59<00:56,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  63%|██████▎   | 154/243 [01:59<00:56,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  64%|██████▍   | 155/243 [02:00<00:56,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  64%|██████▍   | 156/243 [02:01<01:02,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  65%|██████▍   | 157/243 [02:02<01:00,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  65%|██████▌   | 158/243 [02:02<00:57,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  65%|██████▌   | 159/243 [02:03<00:55,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  66%|██████▌   | 160/243 [02:04<00:54,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  66%|██████▋   | 161/243 [02:04<00:53,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  67%|██████▋   | 162/243 [02:05<00:52,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  67%|██████▋   | 163/243 [02:05<00:52,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  67%|██████▋   | 164/243 [02:06<00:51,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  68%|██████▊   | 165/243 [02:07<00:52,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  68%|██████▊   | 166/243 [02:08<00:53,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  69%|██████▊   | 167/243 [02:08<00:52,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  69%|██████▉   | 168/243 [02:09<00:52,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  70%|██████▉   | 169/243 [02:10<00:52,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  70%|██████▉   | 170/243 [02:10<00:51,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  70%|███████   | 171/243 [02:11<00:51,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  71%|███████   | 172/243 [02:12<00:49,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  71%|███████   | 173/243 [02:12<00:48,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  72%|███████▏  | 174/243 [02:13<00:48,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  72%|███████▏  | 175/243 [02:14<00:48,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  72%|███████▏  | 176/243 [02:15<00:46,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  73%|███████▎  | 177/243 [02:15<00:45,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  73%|███████▎  | 178/243 [02:16<00:45,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  74%|███████▎  | 179/243 [02:17<00:44,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  74%|███████▍  | 180/243 [02:17<00:43,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  74%|███████▍  | 181/243 [02:18<00:42,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  75%|███████▍  | 182/243 [02:19<00:41,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  75%|███████▌  | 183/243 [02:19<00:32,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  76%|███████▌  | 184/243 [02:20<00:34,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  76%|███████▌  | 185/243 [02:20<00:35,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  77%|███████▋  | 186/243 [02:21<00:35,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  77%|███████▋  | 187/243 [02:22<00:35,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  77%|███████▋  | 188/243 [02:22<00:35,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  78%|███████▊  | 189/243 [02:23<00:35,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  78%|███████▊  | 190/243 [02:24<00:35,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  79%|███████▊  | 191/243 [02:24<00:34,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  79%|███████▉  | 192/243 [02:25<00:34,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  79%|███████▉  | 193/243 [02:26<00:33,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  80%|███████▉  | 194/243 [02:26<00:32,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  80%|████████  | 195/243 [02:27<00:31,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  81%|████████  | 196/243 [02:28<00:30,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  81%|████████  | 197/243 [02:28<00:29,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  81%|████████▏ | 198/243 [02:29<00:29,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  82%|████████▏ | 199/243 [02:30<00:28,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  82%|████████▏ | 200/243 [02:30<00:28,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  83%|████████▎ | 201/243 [02:31<00:27,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  83%|████████▎ | 202/243 [02:31<00:26,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  84%|████████▎ | 203/243 [02:32<00:25,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  84%|████████▍ | 204/243 [02:33<00:25,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  84%|████████▍ | 205/243 [02:33<00:25,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  85%|████████▍ | 206/243 [02:34<00:24,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  85%|████████▌ | 207/243 [02:35<00:23,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  86%|████████▌ | 208/243 [02:35<00:23,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  86%|████████▌ | 209/243 [02:36<00:23,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  86%|████████▋ | 210/243 [02:37<00:22,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  87%|████████▋ | 211/243 [02:37<00:21,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  87%|████████▋ | 212/243 [02:38<00:20,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  88%|████████▊ | 213/243 [02:39<00:20,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  88%|████████▊ | 214/243 [02:40<00:20,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  88%|████████▊ | 215/243 [02:40<00:20,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  89%|████████▉ | 216/243 [02:41<00:19,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  89%|████████▉ | 217/243 [02:42<00:18,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  90%|████████▉ | 218/243 [02:42<00:16,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  90%|█████████ | 219/243 [02:43<00:14,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  91%|█████████ | 220/243 [02:43<00:13,  1.69it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  91%|█████████ | 221/243 [02:44<00:13,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  91%|█████████▏| 222/243 [02:45<00:12,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  92%|█████████▏| 223/243 [02:45<00:12,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  92%|█████████▏| 224/243 [02:46<00:13,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  93%|█████████▎| 225/243 [02:47<00:12,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  93%|█████████▎| 226/243 [02:48<00:12,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  93%|█████████▎| 227/243 [02:48<00:11,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  94%|█████████▍| 228/243 [02:49<00:11,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  94%|█████████▍| 229/243 [02:50<00:10,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  95%|█████████▍| 230/243 [02:51<00:09,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  95%|█████████▌| 231/243 [02:51<00:08,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  95%|█████████▌| 232/243 [02:52<00:08,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  96%|█████████▌| 233/243 [02:53<00:07,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  96%|█████████▋| 234/243 [02:54<00:06,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  97%|█████████▋| 235/243 [02:54<00:06,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  97%|█████████▋| 236/243 [02:55<00:05,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  98%|█████████▊| 237/243 [02:56<00:04,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  98%|█████████▊| 238/243 [02:57<00:03,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  98%|█████████▊| 239/243 [02:57<00:02,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  99%|█████████▉| 240/243 [02:58<00:02,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references:  99%|█████████▉| 241/243 [02:58<00:01,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references: 100%|█████████▉| 242/243 [02:59<00:00,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Processing references: 100%|██████████| 243/243 [03:00<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_infringement.shape[0]: 970\n",
      "last_token_hidden_states_non_infringement.shape[0]: 243\n",
      "Average Total Time: 0.7300534809910356\n",
      "Average Tpot: 0.005586614325218218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Extracting hidden states for non_infringement texts...\")\n",
    "# X_non_infringement = extract_hidden_states(non_infringement_outputs, model, tokenizer)\n",
    "# print(\"Extracting reference embeddings for non_infringement texts...\")\n",
    "# reference_embeddings_non_infringement = extract_reference_embeddings(non_infringement_references, bert_model, bert_tokenizer)\n",
    "# X_non_infringement_combined = np.hstack([X_non_infringement, reference_embeddings_non_infringement])\n",
    "\n",
    "# print(\"Extracting hidden states for infringement texts...\")\n",
    "# X_infringement = extract_hidden_states(infringement_outputs, model, tokenizer)\n",
    "# print(\"Extracting reference embeddings for infringement texts...\")\n",
    "# reference_embeddings_infringement = extract_reference_embeddings(infringement_references, bert_model, bert_tokenizer)\n",
    "# X_infringement_combined = np.hstack([X_infringement, reference_embeddings_infringement])\n",
    "\n",
    "\n",
    "# 在主程序中\n",
    "print(\"Extracting hidden states for non_infringement texts...\")\n",
    "X_non_infringement = extract_hidden_states(non_infringement_outputs, model, tokenizer)\n",
    "print(\"Extracting decoding for non_infringement texts...\")\n",
    "last_token_hidden_states_non_infringement, totaltime_non_infringement, tpot_non_infringement = extract_decoding(non_infringement_outputs, model, tokenizer, bert_model, bert_tokenizer)\n",
    "\n",
    "if (X_non_infringement.shape[0]!=last_token_hidden_states_non_infringement.shape[0]):\n",
    "    print(\"X_non_infringement.shape[0]:\", X_non_infringement.shape[0])\n",
    "    print(\"last_token_hidden_states_non_infringement.shape[0]:\", last_token_hidden_states_non_infringement.shape[0])\n",
    "    # 对齐两个数组的行数\n",
    "    min_rows = min(X_non_infringement.shape[0], last_token_hidden_states_non_infringement.shape[0])\n",
    "\n",
    "    # 裁剪两个数组到相同的行数\n",
    "    X_non_infringement_aligned = X_non_infringement[:min_rows, :]\n",
    "    last_token_hidden_states_non_infringement_aligned = last_token_hidden_states_non_infringement[:min_rows, :]\n",
    "else:\n",
    "    X_non_infringement_aligned = X_non_infringement\n",
    "    last_token_hidden_states_non_infringement_aligned = last_token_hidden_states_non_infringement\n",
    "\n",
    "    # 进行合并\n",
    "X_non_infringement_combined = np.hstack([X_non_infringement_aligned, last_token_hidden_states_non_infringement_aligned])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting hidden states for infringement texts...\")\n",
    "X_infringement = extract_hidden_states(infringement_outputs, model, tokenizer)\n",
    "print(\"Extracting decoding for infringement texts...\")\n",
    "last_token_hidden_states_infringement, totaltime_infringement, tpot_infringement = extract_decoding(infringement_outputs, model, tokenizer, bert_model, bert_tokenizer)\n",
    "\n",
    "if (X_infringement.shape[0]!=last_token_hidden_states_infringement.shape[0]):\n",
    "    print(\"X_infringement.shape[0]:\", X_infringement.shape[0])\n",
    "    print(\"last_token_hidden_states_non_infringement.shape[0]:\", last_token_hidden_states_infringement.shape[0])\n",
    "    # 对齐两个数组的行数\n",
    "    min_rows = min(X_infringement.shape[0], last_token_hidden_states_infringement.shape[0])\n",
    "\n",
    "    # 裁剪两个数组到相同的行数\n",
    "    X_infringement_aligned = X_infringement[:min_rows, :]\n",
    "    last_token_hidden_states_infringement_aligned = last_token_hidden_states_infringement[:min_rows, :]\n",
    "else:\n",
    "    X_infringement_aligned = X_infringement\n",
    "    last_token_hidden_states_infringement_aligned = last_token_hidden_states_infringement\n",
    "\n",
    "    # 进行合并\n",
    "X_infringement_combined = np.hstack([X_infringement_aligned, last_token_hidden_states_infringement_aligned])\n",
    "\n",
    "# 计算平均值\n",
    "average_totaltime = (totaltime_non_infringement + totaltime_infringement) / 2\n",
    "print(\"Average Total Time:\", average_totaltime)\n",
    "average_tpot = (tpot_non_infringement + tpot_infringement) / 2\n",
    "print(\"Average Tpot:\", average_tpot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into training and test sets.\n"
     ]
    }
   ],
   "source": [
    "split_index_non_infringement = int(0.8 * len(X_non_infringement_combined))\n",
    "X_non_infringement_train = X_non_infringement_combined[:split_index_non_infringement]\n",
    "X_non_infringement_test = X_non_infringement_combined[split_index_non_infringement:]\n",
    "y_non_infringement_train = y_non_infringement[:split_index_non_infringement]\n",
    "y_non_infringement_test = y_non_infringement[split_index_non_infringement:]\n",
    "\n",
    "split_index_infringement = int(0.8 * len(X_infringement_combined))\n",
    "X_infringement_train = X_infringement_combined[:split_index_infringement]\n",
    "X_infringement_test = X_infringement_combined[split_index_infringement:]\n",
    "y_infringement_train = y_infringement[:split_index_infringement]\n",
    "y_infringement_test = y_infringement[split_index_infringement:]\n",
    "\n",
    "X_train = np.vstack((X_non_infringement_train, X_infringement_train))\n",
    "X_test = np.vstack((X_non_infringement_test, X_infringement_test))\n",
    "y_train = np.concatenate((y_non_infringement_train, y_infringement_train))\n",
    "y_test = np.concatenate((y_non_infringement_test, y_infringement_test))\n",
    "\n",
    "print(\"Data successfully split into training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP model with input_dim=4864 and hidden_dim=4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 9/2000 [00:04<17:19,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/2000, Loss: 0.7142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1519, 96]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining MLP model with input_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and hidden_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m custom_mlp, losses, best_accuracy, best_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 38\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_test, y_test, input_dim, hidden_dim, epochs, lr, checkpoint_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msigmoid(y_pred_logits) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and F1-score\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_pred)  \u001b[38;5;66;03m# F1-score calculation\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy at Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/zdh/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/zdh/lib/python3.12/site-packages/sklearn/metrics/_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/zdh/lib/python3.12/site-packages/sklearn/metrics/_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/zdh/lib/python3.12/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1519, 96]"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 4096\n",
    "print(f\"Training MLP model with input_dim={input_dim} and hidden_dim={hidden_dim}\")\n",
    "\n",
    "custom_mlp, losses, best_accuracy, best_f1 = train_model(X_train, y_train, X_test, y_test, input_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to '{filepath}'.\")\n",
    "\n",
    "save_checkpoint(custom_mlp, torch.optim.Adam(custom_mlp.parameters()), len(losses), losses[-1], checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = (torch.sigmoid(torch.tensor(custom_mlp(torch.tensor(X_test, dtype=torch.float32)))) > 0.5).float().numpy()\n",
    "print(classification_report(y_test, y_pred_final, target_names=[\"infringement\", \"non_infringement\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(totaltime_non_infringement)\n",
    "print(tpot_non_infringement)\n",
    "\n",
    "print(totaltime_infringement)\n",
    "print(tpot_infringement)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
