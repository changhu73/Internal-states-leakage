
# # 2.选择不同层：指定layer_index
# def extract_hidden_states(texts, model, tokenizer, layer_index=-1, batch_size=4):
#     hidden_states = []
#     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
#         batch_texts = texts[i:i + batch_size]
#         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
#         with torch.no_grad():
#             outputs = model(**inputs)
#         selected_layer_hidden_state = outputs.hidden_states[layer_index].mean(dim=1).cpu().numpy()
#         hidden_states.append(selected_layer_hidden_state)
#     return np.vstack(hidden_states)

# # 3. 提取多个层的隐藏状态并进行融合, 可以去函数里直接修改layers的元素值，真实测出来会高一些
# def extract_hidden_states(texts, model, tokenizer, layers=None, batch_size=4):
#     hidden_states = []
#     if layers is None:
#         layers = [-4, -3, -2, -1]  # 默认提取最后4层
        
#     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
#         batch_texts = texts[i:i + batch_size]
#         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
#         with torch.no_grad():
#             outputs = model(**inputs)

#         # 提取并融合指定层的隐藏状态
#         selected_layers_hidden_states = [outputs.hidden_states[layer].mean(dim=1) for layer in layers]
#         fused_hidden_state = torch.cat(selected_layers_hidden_states, dim=-1).cpu().numpy()  # 拼接多个层
#         hidden_states.append(fused_hidden_state)
        
#     return np.vstack(hidden_states)

# # 4.隐藏状态的池化策略改进，修改pooling_strategy
# def extract_hidden_states(texts, model, tokenizer, pooling_strategy='cls', batch_size=4):
#     hidden_states = []
#     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
#         batch_texts = texts[i:i + batch_size]
#         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
#         with torch.no_grad():
#             outputs = model(**inputs)
#         layer_hidden_states = outputs.hidden_states[-1]

#         if pooling_strategy == 'mean':
#             pooled_hidden_state = layer_hidden_states.mean(dim=1).cpu().numpy()
#         elif pooling_strategy == 'max':
#             pooled_hidden_state = layer_hidden_states.max(dim=1).values.cpu().numpy()
#         elif pooling_strategy == 'cls':
#             pooled_hidden_state = layer_hidden_states[:, 0, :].cpu().numpy()  # First token (CLS)
#         else:
#             raise ValueError("Unknown pooling strategy")

#         hidden_states.append(pooled_hidden_state)
#     return np.vstack(hidden_states)

# # 5.降维技术（如PCA、t-SNE）来减少特征的维度，或者通过归一化操作使特征更加标准化(apply_pca=True进行降维) 
# from sklearn.decomposition import PCA

# def extract_hidden_states(texts, model, tokenizer, apply_pca=True, n_components=50, batch_size=4):
#     hidden_states = []
#     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
#         batch_texts = texts[i:i + batch_size]
#         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
#         with torch.no_grad():
#             outputs = model(**inputs)
#         hidden_state = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()
#         hidden_states.append(hidden_state)

#     hidden_states = np.vstack(hidden_states)
    
#     if apply_pca:
#         pca = PCA(n_components=n_components)
#         hidden_states = pca.fit_transform(hidden_states)
#         print(f"Hidden states reduced to {n_components} dimensions using PCA.")
        
#     return hidden_states

# 6.TSNE降维
# from sklearn.manifold import TSNE
# def extract_hidden_states(texts, model, tokenizer, apply_tsne=True, n_components=2, batch_size=4):
#     hidden_states = []
#     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
#         batch_texts = texts[i:i + batch_size]
#         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
#         with torch.no_grad():
#             outputs = model(**inputs)
#         hidden_state = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()
#         hidden_states.append(hidden_state)

#     hidden_states = np.vstack(hidden_states)
    
#     if apply_tsne:
#         tsne = TSNE(n_components=n_components, random_state=42)
#         hidden_states = tsne.fit_transform(hidden_states)
#         print(f"Hidden states reduced to {n_components} dimensions using t-SNE.")
        
#     return hidden_states

# # 7.
# import umap

# def extract_hidden_states(texts, model, tokenizer, apply_umap=True, n_components=2, batch_size=4):
#     hidden_states = []
#     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
#         batch_texts = texts[i:i + batch_size]
#         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True)
#         with torch.no_grad():
#             outputs = model(**inputs)
#         hidden_state = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()
#         hidden_states.append(hidden_state)

#     hidden_states = np.vstack(hidden_states)
    
#     if apply_umap:
#         reducer = umap.UMAP(n_components=n_components, random_state=42)
#         hidden_states = reducer.fit_transform(hidden_states)
#         print(f"Hidden states reduced to {n_components} dimensions using UMAP.")
        
#     return hidden_states

















# # def extract_hidden_states(texts, model, tokenizer, batch_size=4):
# #     hidden_states = []
# #     for i in tqdm(range(0, len(texts), batch_size), desc="Processing data batches"):
# #         batch_texts = texts[i:i + batch_size]
# #         inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=1)  # 限制为前5个token
# #         input_ids = inputs['input_ids'][:, :1]  # 取前5个token的ID
# #         attention_mask = inputs['attention_mask'][:, :1]  # 取前5个token的attention mask

# #         with torch.no_grad():
# #             outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)

# #         # 只考虑前5个token的 hidden states，取最后一层
# #         last_hidden_state = outputs.hidden_states[-1][:, :1, :]  # (batch_size, 5, hidden_dim)

# #         # 对前5个token的 hidden states 取均值
# #         hidden_state_mean = last_hidden_state.mean(dim=1).cpu().numpy()  # (batch_size, hidden_dim)
        
# #         hidden_states.append(hidden_state_mean)

# #     return np.vstack(hidden_states)

# def extract_reference_embeddings(references, model, tokenizer, batch_size=4):
#     embeddings = []
#     for i in tqdm(range(0, len(references), batch_size), desc="Processing references"):
#         batch_references = references[i:i + batch_size]
#         inputs = tokenizer(batch_references, return_tensors="pt", padding=True, truncation=True, max_length=1)  # 限制为前5个token
#         input_ids = inputs['input_ids'][:, :1]  # 取前5个token的ID
#         attention_mask = inputs['attention_mask'][:, :1]  # 取前5个token的attention mask

#         with torch.no_grad():
#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)

#         # 对 pooler_output 取前5个token，BERT 的 pooler_output 是整句话的嵌入，但我们只需要对齐前5个token
#         pooler_output = outputs.pooler_output.cpu().numpy()  # (batch_size, hidden_dim)
        
#         embeddings.append(pooler_output)

#     return np.vstack(embeddings)